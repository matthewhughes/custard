<nav class="well">
  <ul class="nav nav-list">
    <li><a data-nonpushstate href="#scraperwiki-intro">Introduction</a></li>
    <li class="nav-header">Datasets</li>
    <li><a data-nonpushstate href="#datasets-basics">Dataset basics</a></li>
    <li><a data-nonpushstate href="#datasets-anatomy">Anatomy of a box</a></li>
    <li><a data-nonpushstate href="#datasets-endpoints">Box API endpoints</a></li>
    <li><a data-nonpushstate href="#datasets-services">Unix services</a></li>
    <li><a data-nonpushstate href="#datasets-git">Using Git with your box</a></li>
    <li class="nav-header">Views</li>
    <li><a data-nonpushstate href="#views-basics">View basics</a></li>
    <li><a data-nonpushstate href="#views-styling">Styling your view</a></li>
    <li><a data-nonpushstate href="#views-helper">Helper library</a></li>
    <li class="nav-header">Tools</li>
    <li><a data-nonpushstate href="#tools-structure">Structure of a tool</a></li>
    <li><a data-nonpushstate href="#tools-process">Development process</a></li>
    <li><a data-nonpushstate href="#tools-submitting">Submitting your tool</a></li>
  </ul>
</nav>

<h2 id="scraperwiki-intro">ScraperWiki is about datasets</h2>

<p>The simplest dataset is just an SQLite database, with a bit of code to create it and possibly update it. </p>

<p>On ScraperWiki, these files live inside what we call a <b>box</b> &ndash; essentially a Unix shell account on the web. Boxes are sandboxed from each other, and provided with a set of API endpoints for communicating with the outside world.</p>

<p>Datasets can be analysed, visualised and exported using <b>views</b>. Each view is a separate box, and communicates with the dataset by making SQL queries and accessing files in the dataset over HTTP.</p>

<p>Datasets and views can be packaged up for you or other people to reuse and customise as <b>tools</b>. Tools are just git repositories, which are automatically checked out into a blank box when they are used to make a dataset or a view.</p>

<hr />

<h2 id="datasets-basics">Dataset basics</h2>

<p>You can <b>create a new dataset</b> by visiting the <a href="/tools">My Tools</a> page, and clicking on the <b>"Code a dataset!"</b> tool.
This will show you how to SSH in.</p>

<p>For an existing dataset, you can use the <b>"View source"</b> tool, on your dataset&rsquo;s overview page, to see the same SSH instructions.</p>

<p class="well well-small"><span class="label label-info">Top tip!</span> If you&rsquo;ve never used SSH before, <a href="http://en.wikipedia.org/wiki/Ssh-keygen">this page about generating SSH keys</a> will come in handy.</p>

<hr />

<h2 id="datasets-anatomy">Anatomy of a box</h2>

<p>A box is a Unix user account on ScraperWiki's server cluster. The Unix user account has the same name as your box (eg: <code>by227hi</code>) and exists inside a <a href="http://en.wikipedia.org/wiki/Chroot">Chroot jail</a> for security and privacy. Your home directory is always <code>/home/</code>. </p>

<p>Because boxes are just Unix user accounts, all your favourite Unix tools like <code>scp</code>, <code>git</code>, and <code>cron</code> work right out of the box.  You have a permanent POSIX filesystem (it uses <a href="http://www.gluster.org/">GlusterFS</a>), limited to 500 MB storage across all the boxes you own. </p>

<p>Box settings, such as the <code>publish_token</code> and default database location, are stored in a JSON file at <code>~/box.json</code>. Eg:</p>

<pre class="prettyprint linenums">{
  "database": "database.sqlite",
  "publish_token": "t5odv7of5l"
}</pre>

<p>Your box&rsquo;s <code>publish_token</code> is used to secure its public-facing, read-only endpoints (<code>/http</code> and <code>/sqlite</code>). It should be alphanumeric and 10-30 characters long. Or, if you&rsquo;d prefer to make your box&rsquo;s contents public, you can set it to an empty string or remove its entry entirely from <code>~/box.json</code>.</p>

<hr />

<h2 id="datasets-endpoints">Box API endpoints</h2>

<p class="well well-small"><span class="label label-important"> Watch out for 403s!</span>We currently only supply <code>Access-Control-Allow-Origin</code> in our API responses, no other CORS headers. Don&rsquo;t preflight your AJAX requests.</p>

<h3 id="datasets-endpoints-http">HTTP file endpoint</h3>

<p>Files placed in the <code>~/http/</code> directory of your box will be served statically via the box&rsquo;s HTTP endpoint. So, a file at <code>~/http/index.html</code> will be accessible at <code>https://box.scraperwiki.com/<em class="muted">&lt;box_name&gt;</em>/<em class="muted">&lt;publish_token&gt;</em>/http/index.html</code></p>

<p>The HTTP file endpoint is a great way to serve static web pages, client-side javascript apps, and downloadable files &ndash; especially when you&rsquo;re writing views, or setup screens for more complex datasets.</p>

<h3 id="datasets-endpoints-sql">SQL data endpoint</h3>

<p>If an SQLite file is named in the <code>database</code> key of <code>~/box.json</code>, you can query it using the read-only SQL endpoint like so: <code>https://box.scraperwiki.com/<em class="muted">&lt;box_name&gt;</em>/<em class="muted">&lt;publish_token&gt;</em>/sql?q=select+*+from+sqlite_master</code>.</p>

<p>The SQL endpoint accepts only <code>HTTP GET</code> requests with the following parameters:</p>

<table class="table table-bordered">
  <thead>
    <tr>
      <th>GET&nbsp;parameter</th>
      <th>Description</th>
    </tr>
  </thead>
  <tr>
    <td><code>q</code></td>
    <td>The SQL query to execute. Multiple queries separated by a <code>;</code> are not allowed.</td>
  </tr>
  <tr>
    <td><code>callback</code></td>
    <td><span class="muted">[optional]</span> A callback function with which to wrap the JSON response, for JSONP output.</td>
  </tr>
</table>

<p>The SQL endpoint returns a JSON list of objects; one object for each row in the result set.</p>

<p>You can also see metadata about your database by making a <code>HTTP GET</code> request to <code>https://box.scraperwiki.com/<em class="muted">&lt;box_name&gt;</em>/<em class="muted">&lt;publish_token&gt;</em>/sql/meta</code>. The meta endpoint returns a JSON object like this:</p>

<pre class="prettyprint">{
  "databaseType": "sqlite3", 
  "table": {
    "deals": {
      "columnNames": [ "deal_id", "ref_no", "deal_name", "status", "created", "updated", "price" ], 
      "type": "table"
    }
  }
}
</pre>

<h3 id="datasets-endpoints-exec">Exec endpoint</h3>

<p>You can execute commands remotely, without SSHing in, by using your box&rsquo;s <code>/exec</code> endpoint. The Exec endpoint accepts <code>HTTP POST</code> requests with two required body parameters:</p>

<table class="table table-bordered">
  <thead>
    <tr>
      <th>POST&nbsp;parameter</th>
      <th>Description</th>
    </tr>
  </thead>
  <tr>
    <td><code>cmd</code></td>
    <td>The Unix command to execute inside the box. Multiple commands separated by a <code>;</code> <em>are</em> allowed. Commands are run from <code>/</code>.</td>
  </tr>
  <tr>
    <td><code>apikey</code></td>
    <td>The API Key of the box owner. (Hint: yours is <code><%= @user.apiKey %></code>)</td>
  </tr>
</table>

<p class="well well-small"><span class="label label-important">Watch out!</span> The Exec endpoint allows potentially destructive access to your box. Never share your API Key with anyone.</p>

<p>Because the Exec endpoint is secured using your <code>apikey</code>, there is no need to provide a <code>publish_token</code> in the URL. Eg:</p>

<pre class="prettyprint linenums">$.ajax({
  url: 'https://box.scraperwiki.com/example/exec', // note: no publish_token
  type: 'POST',
  data: {
    'cmd': 'echo "hello world" > hello.txt; ls -hitlar',
    'apikey': '<%= @user.apiKey %>'
  }
}).done(function(text){
  console.log(text)
})</pre>

<p>Unlike the other box endpoints, the Exec endpoint returns plain text, rather than JSON.</p>

<h3 id="datasets-endpoint-file">File upload endpoint</h3>

<p>Boxes come with a file upload endpoint, allowing you to write datasets or views that accept a user&rsquo;s files as input. The file upload endpoint accepts <code>HTTP POST</code> requests, and like the Exec endpoint, requires your <code>apikey</code> as a body parameter:</p>

<table class="table table-bordered">
  <thead>
    <tr>
      <th>POST&nbsp;parameter</th>
      <th>Description</th>
    </tr>
  </thead>
  <tr>
    <td><code>file</code></td>
    <td>The file you wish to upload.</td>
  </tr>
  <tr>
    <td><code>apikey</code></td>
    <td>The API Key of the box owner. (Hint: yours is <code><%= @user.apiKey %></code>)</td>
  </tr>
  <tr>
    <td><code>next</code></td>
    <td>The URL to which users will be redirected once the files have been uploaded</td>
  </tr>
</table>

<p>You will often use the file upload endpoint as the <code>action</code> attribute of a web form, like so:</p>

<pre class="prettyprint linenums">&lt;!-- in /http/index.html --&gt;
&lt;script src="https://ajax.googleapis.com/ajax/libs/jquery/1.8/jquery.min.js"&gt;&lt;/script&gt;
&lt;script src="https://x.scraperwiki.com/js/scraperwiki.js"&gt;&lt;/script&gt;
&lt;form id="up" action="../../file/" method="POST" enctype="multipart/form-data"&gt;
  &lt;input type="file" name="file" size="80" id="file"&gt;
  &lt;input type="hidden" name="apikey" id="apikey"&gt;
  &lt;input type="hidden" name="next" id="next"&gt;
  &lt;input type="submit" value="Upload now!"&gt;
&lt;/form&gt;
&lt;script&gt;
  settings = scraperwiki.readSettings()
  $('#next').val(window.location.pathname + 'done.html' + window.location.hash)
  $('#apikey').val(settings.source.apikey)
&lt;/script&gt;</pre>

<p>The uploaded file will be put in the <code>/home/incoming/</code> directory.</p>

<h3 id="datasets-endpoints-status">Dataset status API</h3>

<p>The ScraperWiki website can display the status of your datasets (for example, when they last ran, and whether they encountered errors). You can register the status of your dataset by making a <code>HTTP POST</code> request, <em class="text-error">from within the box</em>, to <code>https://x.scraperwiki.com/api/status</code>.</p>

<table class="table table-bordered">
  <thead>
    <tr>
      <th>POST&nbsp;parameter</th>
      <th>Description</th>
    </tr>
  </thead>
  <tr>
    <td><code>type</code></td>
    <td>Your dataset&rsquo;s status. Should either be <code>ok</code> or <code>error</code>.</td>
  </tr>
  <tr>
    <td><code>message</code></td>
    <td><span class="muted">[optional]</span> 
    An optional description, such as <code>Scraped 24 new tweets</code> or <code>Invalid password</code>.
    If not specified, the ScraperWiki website instead shows "Refreshed" or "Error" followed by how long ago
    the status was updated.
    </td>
  </tr>
</table>

<p>This API requires no <code>publish_token</code> or <code>apikey</code> because it automatically detects the credentials of the box from which it&rsquo;s called. Magic!</p>

<p>The endpoint returns a JSON object with a single <code>success</code> key on success, or an <code>error</code> key on errors.</p>

<hr />

<h2 id="datasets-services">Unix services</h2>

<h3>Third party libraries</h3>

<p>We've installed many standard Unix tools.
</p>

<ul>
    <li>Languages such as Python, R, Ruby, PHP, Node, Java and Clojure.</li>
    <li>Scraping libraries such as Mechanize, Hpricot, Highrise, Zombie.</li>
    <li>Data using libraries such as Zombie, NLTK, iCalendar.</li>
    <li>Version control software such as git, Subversion and Mercurial.</li>
    <li>Useful tools like GNU Screen, strace and curl.</li>
    <li>Editors like vim and Emacs.</li>
</ul>

<p>You can install anything else you need in the box. Use a language-specific
package manager (e.g. pip, gem) or download the source and compile it. For
example, to get a Python package, do this:

<pre class="prettyprint">pip install --user &lt;packagename&gt;
</pre>

<h3>Cron</h3>

<p>You can create a standard cron job using the <code>crontab</code> command.
The <code>MAILTO</code> variable works as normal, and is a good way to get
cron output via email.</p>

<p>Unlike other facilities, this runs separately on each physical server in
ScraperWiki's cluster. Currently you only have access to one server &ndash; this will
change as we add more.</p>

<h3>SMTP</h3>

<p>You can send email using SMTP via port 25 on <code>localhost</code>. This is intended
for logging and alerting. Spam is against our Terms &amp; Conditions, and will not work.</p>

<h3>SSH</h3>

<p>This is an ordinary SSH server, you can use <code>scp</code>,
<code>sftp</code>, <code>git</code> over SSH and so on. Your keys are stored
separately for each box in <code>/home/.ssh/</code>, so you can add and
remove people.</p>

<hr />

<h2 id="datasets-git">Using Git with your box</h2>

<p>Every box contains a Git repo in the <code>~/tool/</code> directory. 
You can either work on it on your laptop (locally) or in the box (remotely).</p>

<h3>Local development</h3>

<p>To develop code locally and push it to your box using Git, you need this
configuration setting inside the box.</p>

<pre class="prettyprint">git config --global receive.denyCurrentBranch "ignore"
</pre>

<p>You can then clone to your local machine using:</p>

<pre class="prettyprint">git clone a1b2c3d@box.scraperwiki.com:tool ./tool</pre>

<p>Where <code>a1b2c3d</code> is the name of your box, and <code>./local-directory-name</code> is the directory in which you&rsquo;d like the local repo created.</p>

<p>The box will not automatically update when you push your changes to it. You
can either SSH in and run <code>git reset --hard</code> manually, or set an
automatic post hook inside the box:</p>

<pre class="prettyprint">echo "cd ..; env -i git reset --hard" > ~/tool/.git/hooks/post-receive
chmod +x ~/tool/.git/hooks/post-receive</pre>

<h3>Remote development</h3>

<p>You can SSH into the box, edit the code there, and commit it.
Git needs to know who you are. To automatically tell it for every box, add this
to your local <code>~/.ssh/config</code>.
</p>

<pre class="prettyprint">SendEnv LANG LC_* EDITOR GIT_*</pre>

<p>And add this to your local <code>~/.bashrc</code> or equivalent.

<pre class="prettyprint">export GIT_AUTHOR_EMAIL=<%= @user.email[0] %>
export GIT_AUTHOR_NAME='<%= @user.displayName %>'
export GIT_COMMITTER_EMAIL=<%= @user.email[0] %>
export GIT_COMMITTER_NAME='<%= @user.displayName %>'</pre>

<p>SSH will then pass those environment variables through, and Git will
use them to record when you make commits inside the box. 

<p class="well well-small"><span class="label label-info">Top tip!</span>
You can pass any environment variables you like to your box using <code>SendEnv</code>.
For example, use <code>EDITOR</code> to get the text editor that you're used
to. Make sure you <code>export</code> the variable.
</p>

<p>For less important code, just use version control locally inside
the box. For more important code, add a remote and push/pull to it.
Full instructions are in Github's help on
<a href="https://help.github.com/articles/create-a-repo">Creating a repository</a>.
In short, do this:
</p>

<pre class="prettyprint">git remote add origin https://github.com/username/teach-example-tool.git
git push origin master</pre>

<p>You'll find you need to keep typing in your Github password, or set up
a new SSH private key for each box. Agent forward is a much better and
easier solution. In short, set up a local SSH agent, and add this to <code>~/.ssh/config</code>:
</a>

<pre class="prettyprint">Host box
HostName box.scraperwiki.com
ForwardAgent yes</pre>

<p>Full instructions are in Github's help on
<a href="https://help.github.com/articles/using-ssh-agent-forwarding">Using SSH agent forwarding</a>.
</p>

<p class="well well-small"><span class="label label-info">Top tip!</span>
As a bonus, you can also then simply do <code>ssh &lt;box_name&gt;@box</code> to connect to any box.
</p>

<hr />

<h2 id="views-basics">View basics</h2>

<p>You can <b>create a new view</b> clicking on the <b>"Code a view!"</b> tool on a dataset overview page. This will show you how to SSH in.</p>

<p class="well well-small"><span class="label label-info">Top tip!</span> You can find out how to SSH into any dataset or view by looking for its seven character identifier in the URL, and using it as the user name in your SSH command.</p>

<p>A view is just HTML, starting at the file <code>http/index.html</code>. Visualisations are a combination of HTML, Javascript, and calls to run arbitary commands inside the box if you need them. The view loads inside an <code>iframe</code>.
</p>

<hr />

<h2 id="views-styling">Styling your view</h2>

<p>It's important that views share the same styling and fit in with the rest of ScraperWiki.
You'll want to include Bootstrap and our custom style sheet.
</p>

<pre class="prettyprint">&lt;link rel="stylesheet" href="http://x.scraperwiki.com/vendor/style/bootstrap.min.css"&gt; 
&lt;link rel="stylesheet" href="http://x.scraperwiki.com/vendor/style/metro.bootstrap.css"&gt; 
&lt;script src="https://ajax.googleapis.com/ajax/libs/jquery/1.8/jquery.min.js"&gt;&lt;/script&gt; 
&lt;script src="http://x.scraperwiki.com/vendor/js/bootstrap.min.js"&gt;&lt;/script&gt;</pre>

<p>Write your HTML in a way that <a href="http://twitter.github.com/bootstrap/base-css.html">works
well with Boostrap</a>.
</p>

<hr />

<h2 id="views-helper">Helper library</h2>

<p>There's a client side Javascript library, which wraps a lot of the API
endpoints and functions needed by views. You'll want to include it in
your view's HTML <code>&lt;head&gt;</code>.</p>

<pre class="prettyprint">&lt;script src="https://x.scraperwiki.com/js/scraperwiki.js"&gt;&lt;/script&gt;</pre>

<h3>Read settings</h3>

<p>When somebody loads your view, ScraperWiki passes it a number of settings. The settings are
in a URL encoded JSON object after the # in the URL. You can use this helper function
to access them easier:

<pre class="prettyprint">
scraperwiki.readSettings()
</pre>

<p>This will return a JSON object where <code>target</code> contains information about the box of the dataset being visualised
and <code>source</code> contains information about the view's box.
</p>

<pre class="prettyprint linenums">{
  "source": {
    "apikey": "<%= @user.apiKey %>",
    "publishToken": "t5odv7of5l",
    "box": "fdtlza1",
    "url": "https://box.scraperwiki.com/fdtlza1/t5odv7of5l"
  }, 
  "target": {
    "publishToken": "a1pax8jk32",
    "box": "de3jikz",
    "url": "https://box.scraperwiki.com/de3jikz/a1pax8jk32"
  }
}</pre>

<p>For example, this gets the target dataset's <a data-nonpushstate href="#datasets-endpoints-sql">SQL data endpoint</a>.</p>

<pre class="prettyprint">
scraperwiki.readSettings().target.url + "/sqlite"
</pre>

<h3>Read box name</h3>

<p>The name (random letters and numbers) of the current box is in <code>scraperwiki.box</code>.</p>

<h3>Redirecting</h3>

<p>Since you're in a secure iframe, you need to call a special function to 
redirect the browser to another location. For example, this redirects to
the dataset overview page:
</p>

<pre class="prettyprint linenums">var datasetUrl = "/dataset/" + scraperwiki.box
scraperwiki.tool.redirect(datasetUrl)</pre>

<p>It's also useful for redirecting to OAuth endpoints.</p>

<h3>Read the URL</h3>

<p>Sometimes you will need the URL of the outer page, for example to
read query parameters or to get a URL for OAuth to redirect back to.
Getting the URL happens asynchronusly using <a href="http://easyxdm.net/wp/">XDM</a>,
so you need to pass in a callback function.
</p>

<pre class="prettyprint linenums">scraperwiki.tool.getURL(function(url) {
  console.log(url)
})</pre>

<h3>Rename a dataset / view</h3>

<p>By default, datasets or views created by a tool adopt the name of the tool itself. You can change the name using the <code>scraperwiki.tool.rename()</code> function. It takes one argument: a string to which the dataset or view should be renamed. For example:</p>

<pre class="prettyprint linenums">var username = "@scraperwiki"
scraperwiki.tool.rename(username + "'s twitter followers")</pre>

<h3>SQL API wrapper</h3>

<p>The ScraperWiki helper library includes a convenient wrapper around the target box&rsquo;s SQL API: <code>scraperwiki.sql()</code>. The function takes three arguments:</p> 

<table class="table table-bordered">
  <thead>
    <tr>
      <th>Argument</th>
      <th>Description</th>
    </tr>
  </thead>
  <tr>
    <td><code>command</code></td>
    <td>The SQL query to execute in the target box.</td>
  </tr>
  <tr>
    <td><code>success(data, textStatus, jqXHR)</code></td>
    <td>A callback to run on success.</td>
  </tr>
  <tr>
    <td><code>error(jqXHR, textStatus, errorThrown)</code></td>
    <td>A callback to run on error.</td>
  </tr>
</table>

<p>Example usage:</p>

<pre class="prettyprint linenums">scraperwiki.sql("select * from tweets", function(data, textStatus, jqXHR) {
    console.log('Great! Here are your tweets:', data)
}, function(jqXHR, textStatus, errorThrown){
    console.log('Oh no! Error:', jqXHR.responseText, textStatus, errorThrown)
})</pre>

<p>The SQL metadata API also has a wrapper, which takes a <code>success</code> and <code>error</code> callbacks, like the main SQL function above. Example usage:</p>

<pre class="prettyprint linenums">scraperwiki.sql.meta(function(metadata, textStatus, jqXHR) {
  for(tableName in metadata.table){
    console.log('table:', tableName, 'columns:', metadata.table[tableName].columnNames)
  }
}, function(jqXHR, textStatus, errorThrown){
  console.log('Oh no! Error:', jqXHR.responseText, textStatus, errorThrown)
})
</pre>

<h3>Exec API wrapper</h3>

<p>The ScraperWiki helper library also includes a convenient wrapper around the <em>source</em> box&rsquo;s Exec API: <code>scraperwiki.exec()</code>. The function takes three arguments:</p> 

<table class="table table-bordered">
  <thead>
    <tr>
      <th>Argument</th>
      <th>Description</th>
    </tr>
  </thead>
  <tr>
    <td><code>command</code></td>
    <td>The shell command to execute in the <em>source</em> box.</td>
  </tr>
  <tr>
    <td><code>success(data, textStatus, jqXHR)</code></td>
    <td>A callback to run on success.</td>
  </tr>
  <tr>
    <td><code>error(jqXHR, textStatus, errorThrown)</code></td>
    <td>A callback to run on error.</td>
  </tr>
</table>

<p>Example usage:</p>

<pre class="prettyprint linenums">scraperwiki.exec("cd; ./code.py", function(data, textStatus, jqXHR) {
    console.log('Code.py exited successfully:', data)
}, function(jqXHR, textStatus, errorThrown){
    console.log('Oh no! Error:', jqXHR.responseText, textStatus, errorThrown)
})</pre>

<h3>Escaping strings for shell commands</h3>

<p><code>scraperwiki.shellEscape()</code> returns a string with single-quotes escaped (more for convenience than security). This will come in useful when you&rsquo;re executing commands via the <code>scraperwiki.exec()</code> method.</p>

<h3>Show alerts</h3>

<p><code>scraperwiki.alert()</code> takes three arguments and displays an alert bar at the top of your dataset / view:</p>

<table class="table table-bordered">
  <thead>
    <tr>
      <th>Argument</th>
      <th>Description</th>
    </tr>
  </thead>
  <tr>
    <td><code>title</code></td>
    <td>A string to be shown in bold at the start of the alert. eg: "Could not authenticate with Twitter"</td>
  </tr>
  <tr>
    <td><code>message</code></td>
    <td>A string to be shown after the title, usually to explain the alert or give the user some help. eg: "Please double-check your username and password"</td>
  </tr>
  <tr>
    <td><code>level</code></td>
    <td><span class="muted">[optional]</span> An optional boolean to control the appearance of the alert. True values will cause the alert to be shown in red (<code>.alert-error</code>); false values will cause it to be shown in the default yellow.</td>
  </tr>
</table>

<p>Example usage:</p>

<pre class="prettyprint">scraperwiki.alert("Could not authenticate with Twitter", "Please check your username and password", 1)</pre>

<hr />

<h2 id="tools-structure">Structure of a tool</h2>

<p>Tools are packaged datasets or views &ndash; files in a git repository which are built to run automatically when installed into a new box. By way of example, here are the contents of the <a href="https://github.com/scraperwiki/spreadsheet-upload-tool">Spreadsheet upload tool</a>:</p>

<pre class="prettyprint">code/
 └ extract.py
http/
 ├ done.html
 ├ index.html
 └ style.css
test/
 ├ simple.py
 └ tricky.py
README.md
scraperwiki.json</pre>

<p>When someone activates the Upload Spreadsheet tool, ScraperWiki creates a new box, checks the Git repository out into the box&rsquo;s <code>/home/tool/</code> directory, and shows the tool&rsquo;s <code>http/index.html</code> file to the user in an iframe. This <code>index.html</code> contains javascript that reads settings and generates a user interface for selecting a spreadsheet to upload. It works just like the <a href="views-basics">views</a> described above, and uses the <a href="#datasets-endpoints">box API endpoints</a> to select data and run server-side code.</p>

<p>The <code>scraperwiki.json</code> file contains the following settings describing the tool:</p>

<pre class="prettyprint linenums">{
  "displayName": "Upload a Spreadsheet",
  "gitUrl": "git://github.com/scraperwiki/spreadsheet-download-tool.git",
  "description": "Upload an Excel file or CSV"
}</pre>

<p>A <code>README.md</code> file is included in the Git repo so <a href="https://github.com/scraperwiki/spreadsheet-upload-tool">collaborators on Github</a> know what the tool does. This file is not read by ScraperWiki. You might want to put technical configuration instructions in here.</p>

<p>The <code>test/</code> directory contains Python unit tests used during development. We suggest you write tests for your tools, expecially when listing them publically in the ScraperWiki Data Store.</p>

<hr />

<h2 id="tools-process">Development process</h2>

<p>When you&rsquo;re writing a new tool on your local machine, you&rsquo;ll probably want to preview your changes without officially committing them to the git repo. Since tools rely so heavily on the ScraperWiki platform, previewing locally isn&rsquo;t an option. But neither is comitting every change you make and initiating the tool again and again via the ScraperWiki web interface.</p>

<h3>Local development</h3>

<p>It&rsquo;s for this reason that we&rsquo;ve built <a href="https://github.com/scraperwiki/swot">swot</a>: a tool for developing tools! Install it from our git repo:</p>

<pre>git clone git://github.com/scraperwiki/swot.git</pre>

<p>You will need a box to test your dataset or view in. Create one on ScraperWiki by using the "Code your own dataset!" or "Code your own view!" tool. You will be shown SSH instructions for the resulting box &ndash; make a note of the 7-character <code>box_name</code> and your <code>apikey</code>, and enter them into <code>swot</code> from within your local tool directory:</p>

<pre>swot setup <em class="muted">&lt;box_name&gt;</em> <em class="muted">&lt;apikey&gt;</em></pre>

<p>This automatically adds your SSH keys to the box, and saves some settings into <code>.swotconfig</code></p>

<p>To get <code>swot</code> to watch your local directory for changes and automatically sync them up to the box on ScraperWiki, run:</p>

<pre>swot watch</pre>

<p>Note: a bug in node.js means this doesn&rsquo;t work on a Mac. Try manually synchronizing instead:</p>

<pre>swot sync</pre>

<p>Your changes will now be visible through the normal ScraperWiki interface, at <code>http://x.scraperwiki.com/dataset/<em class="muted">&lt;dataset_box&gt;</em>/settings</code> if you are working on a dataset, or <code>http://x.scraperwiki.com/dataset/<em class="muted">&lt;dataset_box&gt;</em>/view/<em class="muted">&lt;view_box&gt;</em></code> if you are working on a view.</p>

<h3>Remote development</h3>

<p>If you&rsquo;d prefer to edit the live code directly in a box, you can SSH in, and use <code>vim</code> or <code>emacs</code>. You&rsquo;ll need to configure your environment there, separately for each tool you&rsquo;re making.</p>

<hr />

<h2 id="tools-submitting">Submitting your tool</h2>

<p>To sumbit a completed tool (or work in progress!) to ScraperWiki, make a <code>HTTP POST</code> request to <code>/api/tools</code>, with the following body parameters:</p>

<table class="table table-bordered">
  <thead>
    <tr>
      <th>POST&nbsp;parameter</th>
      <th>Description</th>
    </tr>
  </thead>
  <tr>
    <td><code>name</code></td>
    <td>A unique name for your tool. Should contain only alphanumeric characters and hyphens. This internal to ScraperWiki and will not be seen by end-users.</td>
  </tr>
  <tr>
    <td><code>type</code></td>
    <td>Either <code>importer</code> if your tool creates new datasets, or <code>view</code> if it visualises/exports existing datasets.</td>
  </tr>
  <tr>
    <td><code>gitUrl</code></td>
    <td>URL of your tool&rsquo;s Git repo, eg: <code>git://github.com/scraperwiki/spreadsheet-download-tool.git</code></td>
  </tr>
</table>

<p>This API endpoint currently requires an active ScraperWiki login session. It is therefore easiest to call the API from the Javascript console in your web browser, while you are logged into ScraperWiki. eg:</p>

<pre class="prettyprint">
$.ajax({url: "/api/tools", type: "POST", data: {name: "spreadsheet-download", type: "view", gitUrl: "git://github.com/scraperwiki/spreadsheet-download-tool.git"}}).complete(function(r){ console.log(r.responseText) })
</pre>

<p>ScraperWiki will check out your Git repo, and extract additional information about your tool from the <code>scraperwiki.json</code> file inside. Please make sure your tool follows <a href="#tools-structure">the structure outlined above</a>.</p>

<p class="well well-small"><span class="label label-important">Private repo?</span> No worries! Make a new Github user with <span class="text-error">read-only access</span> to your repository, and then give us a <code>gitUrl</code> in the following format: <code>https://user:password@github.com/…</code></p>

<p>ScraperWiki always pulls the latest code from the specified <code>gitUrl</code> when users use your tool. Datasets or views they have already created, however, will not be automatically updated.</p>
